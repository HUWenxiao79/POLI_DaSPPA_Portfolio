---
title: "Tokenization, Wrangling, EDA"
author: "Haohan Chen (HKU)"
date: "`r Sys.Date()`"
output: html_document
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Introduction

This notebook demonstrate tokenization, basic text wrangling, and exploratory data analysis.

## Further Readings

Please refer to the assigned readings for the text mining section. For this lecture, pay close attention to the following chapters

-   Chapter 1. The Tidy text format (<https://www.tidytextmining.com/tidytext.html>)

-   Chapter 3. Analyzing word and document frequency: tf-idf (<https://posit.cloud/spaces/327904/content/5624913>)

```{r}
library(tidyverse)
library(lubridate)
```

```{r}
d_fulltext = read_rds("Lecture_10/1_Text_Mining/data/data_ce_speech_article.rds")
# Change the date variable to "date" format
d_fulltext = d_fulltext |> mutate(date_of_speech = dmy(date_of_speech))
```

## Tokenization

```{r}
if (!require("tidytext")) install.packages("tidytext")

library(tidytext) # Full introduction: http://tidytextmining.com/
```

```{r}

d_fulltext <- d_fulltext |>
  mutate(text = str_replace_all(text, "Hong Kong", "HongKong"))

d_tokenized = d_fulltext |>
  select(uid, date_of_speech, text) |>
  unnest_tokens(word, text)
# first arg: output name; second: input name

head(d_tokenized, 20)

# Simple?
```

## Wrangling: Remove Stop Words

```{r}
# Load Stopwords
data("stop_words")

head(stop_words, 20)
```

```{r}
# Remove stopwords
d_tokenized_s = d_tokenized |>
  anti_join(stop_words, by = "word")
# anti_join: whatever appearing in the stop_words dataframe, we remove it.
```

## Wrangling [Optional]: Stemming

```{r}
if (!require(SnowballC)) install.packages("SnowballC")
library(SnowballC)
```

```{r}
d_tokenized_s = d_tokenized_s |>
  mutate(stem = wordStem(word))

head(d_tokenized_s, 20)
```

## Exploratory Data Analysis

### Count word frequencies

```{r}
# Count term frequencies (for raw words)
word_frequency = d_tokenized_s |>
  count(word, sort = TRUE)

head(word_frequency, 20)

# Count term frequencies (for Stemmed word -- recommended)
word_frequency = d_tokenized_s |>
  count(stem, sort = TRUE) |>
  rename("word" = "stem")

head(word_frequency, 20)

#remove number
#word_frequency <-word_frequency |>
  #filter(str_detect(word,"[0-9]+"))
#seems that something is lacking
```

### Examine most popular words

```{r}
# Get a subset of most frequent words
word_frequency_top = word_frequency |>
  arrange(desc(n)) |> # Make sure that it is sorted properly
  slice(1:200) # Take the first 200 rows. 
```

### Plot most popular words

```{r}
word_frequency_top |>
  slice(1:10) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  theme_bw()
```

### Plot a Word Cloud

```{r}
if (!require(ggwordcloud)) install.packages("ggwordcloud")
library(ggwordcloud)

word_frequency_top |>
  slice(1:100) |>
  ggplot(aes(label = word, size = n)) +
  scale_size_area(max_size = 10) +
  geom_text_wordcloud() +
  theme_minimal()

#can change the slice and max_size
```

```{r}
# An alternative wordcloud package
if (!require(wordcloud)) install.packages("wordcloud")
library(wordcloud)

wordcloud(
  word_frequency_top$word, word_frequency_top$n, 
  rot.per = 0, random.order = FALSE, random.color = TRUE)

```

```{r, results='hide'}
# The third wordcloud package
# https://r-graph-gallery.com/196-the-wordcloud2-library.html
if (!require(wordcloud2)) install.packages("wordcloud2")
library(wordcloud2)

wordcloud2(word_frequency_top)

wordcloud2(word_frequency_top, shape = "star")

wordcloud2(word_frequency_top, shape = "pentagon")

```

## Comparative Exploratory Analysis

How does the focus differ between 2021 and 2020? Our final set of analysis in this note focuses on a comparative analysis of word frequencies.

```{r}
# Calculate term frequencies for 2020 and 2021 respectively
word_frequency_compare_21_20 = 
  d_tokenized_s |>
  mutate(year = year(date_of_speech), .after = "date_of_speech") |>
  # Extract the year of the speech
  filter(year == 2020 | year == 2021) |>
  group_by(year, stem) |>
  count(sort = TRUE) |>
  pivot_wider(names_from = "year", values_from = "n", 
              names_prefix = "n_", values_fill = 0) |>
  ungroup() |>
  mutate(
    prop_2021 = n_2021 / sum(n_2021),
    prop_2020 = n_2020 / sum(n_2020)
  )
    
```

```{r}
# Visualize the word frequencies in the two years
word_frequency_compare_21_20 |>
  ggplot(aes(x = prop_2020, y = prop_2021)) +
  geom_point()

word_frequency_compare_21_20 |>
  ggplot(aes(x = prop_2020, y = prop_2021)) +
  geom_point() +
  scale_x_sqrt() + scale_y_sqrt()


word_frequency_compare_21_20 |>
  ggplot(aes(x = log(prop_2020), y = log(prop_2021))) +
  geom_point()

word_frequency_compare_21_20 |>
  filter(n_2020 >= 10) |>
  ggplot(aes(x = log(prop_2020), y = log(prop_2021))) +
  geom_point() +
  geom_smooth()

```

```{r}
# The biggest difference?

## What are the words that feature 2020 speeches
tmp_plot_20 = word_frequency_compare_21_20 |>
  mutate(diff = prop_2020 - prop_2021) |>
  slice_max(diff, n = 30) |>
  arrange(desc(diff))
  
## What are the words that feature 2021 speeches
tmp_plot_21 = word_frequency_compare_21_20 |>
  mutate(diff = prop_2021 - prop_2020) |>
  slice_max(diff, n = 30) |>
  arrange(desc(diff))

```

```{r}
# Visualize the difference in a nice way?
set.seed(327)
tmp_plot_merge = tmp_plot_21 |> 
  mutate(Year = "2021") |>
  bind_rows(
    tmp_plot_20 |> mutate(Year = "2020")
    ) 

tmp_plot_merge |>
  ggplot(aes(label = stem, x = Year, color = Year, size = abs(diff))) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 14) +
  theme_minimal() +
  theme(legend.position = "top")

tmp_plot_merge |>
  ggplot(aes(label = stem, y = Year, color = Year, size = abs(diff))) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 14) +
  theme_minimal() +
  theme(legend.position = "top")


```
